Course: Kubernetes for DevOps Engineers. 
Tutor: Samuel
Date: June 21, 2025
Lesson: Introduction to Kubernetes
Day: 1
Class Starts: 7:15pm GMT

# Introduction to Kubernetes

What is Kubernetes?
Kubernetes, often abbreviated as K8s, is an open-source platform designed to automate the deployment, 
scaling, and management of containerized applications. Originally developed by Google and now maintained 
by the Cloud Native Computing Foundation (CNCF), Kubernetes has become the industry standard for container orchestration.

At its core, Kubernetes provides a framework to run distributed systems resiliently. 
It handles the work of scheduling containers onto a compute cluster and manages workloads 
to ensure they run as the user intends. Kubernetes abstracts the underlying infrastructure, 
allowing developers and engineers to focus on building applications rather than managing hardware.

- The Problem Kubernetes Solves
Before Kubernetes, deploying applications at scale was a complex and error-prone process. Organizations faced several challenges:
1. Manual Deployment and Scaling: Without automation, deploying applications across multiple servers required significant manual effort. Scaling up or down to meet demand was slow and inefficient.
2. Resource Utilization: Optimizing resource usage was difficult. Servers often ran under capacity, wasting computational resources and increasing costs.
3. Application Resilience: Ensuring high availability and fault tolerance demanded complex configurations. Recovering from failures required manual intervention, leading to downtime.
4. Environment Consistency: Discrepancies between development, testing, and production environments led to the classic "it works on my machine" problem, causing deployment issues.
5. Complex Networking and Service Discovery: Configuring networking between services was intricate, and enabling services to discover each other dynamically was challenging.

- Kubernetes addresses these problems by providing:
1. Automated Deployment and Scaling: It automates the distribution of application containers across a cluster and adjusts the number of running containers based on demand.
2. Efficient Resource Management: Kubernetes schedules workloads based on resource requirements and constraints, maximizing utilization.
3. Self-Healing Mechanisms: It automatically restarts failed containers, replaces nodes, and kills unresponsive containers to maintain desired state.
4. Consistent Environments: Containers encapsulate applications and their dependencies, ensuring consistency across different environments.
5. Simplified Networking and Service Discovery: Kubernetes offers built-in service discovery and load balancing, simplifying communication between services.

- How Kubernetes Makes Life Easier for Developers and Engineers

For Developers:
1. Focus on Application Development: Developers can concentrate on writing code without worrying about the underlying infrastructure or deployment complexities.
2. Consistent Deployment Environment: Containers ensure that applications run the same way in development, staging, and production, reducing environment-related bugs.
3. Simplified CI/CD Pipelines: Kubernetes integrates well with continuous integration and continuous deployment tools, streamlining the release process.
4. Scalable Applications: Applications can handle increased load seamlessly, as Kubernetes manages scaling automatically.

For Operations Engineers:
1. Automated Operations: Routine tasks like deployment, scaling, and updates are automated, reducing manual workload and the potential for human error.
2. Efficient Resource Utilization: Kubernetes optimizes the use of infrastructure resources, which can lead to cost savings.
3. High Availability and Resilience: Built-in self-healing and replication features ensure that applications remain available, even in the face of failures.
4. Flexibility and Portability: Kubernetes runs on various environments—on-premises, cloud, or hybrid setups—providing flexibility in infrastructure choices.
5. Enhanced Security: Kubernetes offers features like secrets management and role-based access control (RBAC), helping to secure applications and infrastructure.


First Steps
- Prerequisites - Linux, Git and GitHub, Basic AWS Skills. 


Monolithic Application - Entire application as one 
Microservices - Break down into multiple services


The Containers Revolution 
- Docker and Containers


- Kubernetes in Action 
Apps in Kubernetes are called Pods 
The internal loadbalancer are called Services
The front-facing loadbalancer which acts as a router is called Ingress. 

- Creating Resources in Kubernetes 
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/

- Kubernetes Resources in YAML 
YAML = Yet Another Markup Language 

JSON 

Eg. of JSON 
eg.json 
{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "my-first-pod"
  },
  "spec": {
    "containers": [
      {
        "name": "web",
        "image": "my-container",
        "ports": [
          {
            "containerPort": 80
          }
        ]
      }
    ]
  }
}

eg.yml or eg.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: my-first-pod
spec:
  containers:
    - name: web
      image: my-container
      ports:
        - containerPort: 80


apiVersion: 
kind: 
metadata: 
spec: 

https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

MacUsers: brew install kubectl 

https://kubernetes.io/docs/tasks/tools/

Summary 
- Pods (the applications/containers)
- Services (the internal loadbalancers)
- Ingresses (the external loadbalancers)
- Deployments (the component that monitors the Pods)

# Recap - Infrastructure, the past, present and future
- Managing applications at scale is challenging.
- Maintaining and running infrastructure for thousands of applications can be even more challenging.

- Virtual machines are an excellent mechanism to isolate workloads, but they have limitations:
1. Resources are allocated upfront. Every CPU cycle or megabyte of memory that you don't use is lost.
2. Each virtual machine runs an operating system that consumes memory and CPU. The more virtual machines, the more resources you have to spend to keep operating systems running.
3. Virtual machines emulate even the hardware — even if you don't need it. A virtual machine is a virtual computer that emulates network drivers, CPU, storage, etc.
4. You should invest in tooling and processes to create, run and maintain virtual machines. 

- The industry hasn't settled on a single tool or strategy, so there is little by way of an off the shelf solution that you can leverage.

But are virtual machines the only mechanism to isolate workloads?

- Virtual machines are not the only technology capable of isolating and segregating apps.
- Linux containers are a lightweight alternative that restrict what a process can do. dotcloud. 
- Containers leverage primitives in the Linux kernel to isolate processes on the same operating system.
  
- Docker is a tool that builds on top of those abstractions and offers a developer-friendly interface to run processes as containers.
- It also offers a mechanism to build a container image, as well as to share them.
- Running containers doesn't require as many resources as running a virtual machine since there's no hardware to virtualise.
- Developers can benefit from containers since they can run the same dependency as production, but locally.
- No wonder more and more applications are packaged as containers!

However, what happens when there are so many applications packaged as containers that you can't run them in a single server?

Could you have containers deployed across several hosts?
- Unfortunately, Linux containers are designed to encapsulate processes only.
- They don't define how two processes on two different servers should talk to each other.
- When you want to manage a large number of containers across several servers, you should consider using a container orchestrator.


# Managing containers allocations & placements
- When you have hundreds if not thousands of containers, you should find a way to run the containers on multiple servers.
- And it would be best if you could spread the load. You have to ensure high availability (HA)
- When you can distribute the load across several nodes, you can prevent a single failure from taking down the entire service.
- You can also scale your application as you receive more traffic.
- When you run multiple servers, the containers should be able to connect and exchange messages with each other.
- However, Linux containers are designed only to restrict what a process can do.
- They don't prescribe a way to connect multiple processes across several hosts.

# And Kubernetes was born. 2014 
- You can think of Kubernetes as three things:
1. A single computer that abstracts your data centre.
2. A scheduler.
3. A unified API layer over your infrastructure.


# Kubernetes embraces the idea of treating your servers as a single unit and abstracts away how individual compute resources operate.
- From a collection of multiple servers, Kubernetes makes a single cluster that behaves like one server.
- Dealing with a single computer is more straightforward than coordinating deployments across a fleet of servers.
- When you install Kubernetes in your infrastructure, all servers are connected.
- Kubernetes isn't picky. You can give it any computer that has memory and CPU, and it will use them to run your workloads.
- CPU and memory for each compute resource are stored in Kubernetes and used to decide how to allocate containers.

# Kubernetes as a scheduler
- When you deploy an application, Kubernetes identifies the memory requirements for your container and finds the best place to deploy your application.
- You don't decide which server the application is deployed into.
- Kubernetes has a scheduler that decides it for you.
- The scheduler is optimised to pack containers efficiently:
- Initially, it spreads containers over the servers to maximise resiliency.
As time goes on, the scheduler will try to fill containers in the same host, while still trying to balance resource utilisation.

# All you need to know about Kubernetes in General
- Kubernetes is a platform to run distributed applications resiliently.
- If you have a collection of servers and a few apps you wish to deploy, this is what Kubernetes can do for you:
1. It can run and look after your apps packaged as containers. If a container crashes or is deleted, Kubernetes automatically respawns a new one.
2. It can schedule containers efficiently and maximise CPU and memory utilisation of your servers.
   SQL SEQUEL 
   NoSQL eg. MongoDB, DynamoDB, etc
3. It provides a unified network across your fleet of servers so that containers can communicate no matter where they are deployed.
4. It provides the basic building blocks to build scalable services such as autoscalers and load balancers to distribute the traffic.
5. It bundles a mechanism to roll out updates with zero downtime. If the deployment fails, Kubernetes can roll back your deployment to the previous working version.

- As your application increases in scope, Kubernetes can do even more:
1. It offers a standard storage interface that containers can consume to save and retrieve data.
2. It lets you store and manage sensitive information securely, such as passwords, tokens, and keys.
3. It can segregate access to resources and assign roles to users and apps. RBAC
4. It can inspect, validate and reject deployments based on custom and internal rules. 
5. It can collect and aggregate metrics for monitoring and reporting. <Prometheus and Grafana>


# Deploying apps on Kubernetes
The rest of this course depends entirely on yaml.
YAML is designed to be easy for humans to read and write, while still being easy for machines to parse and generate. 
It uses indentation (spaces) to denote structure, and supports complex data structures such as lists, 
associative arrays (known as "dictionaries" or "hashes"), and scalar types (boolean values, integers, 
floating point numbers, strings, null, etc.).
It is a data serialisation standard used in configuration files, deployment scripts, and in applications 
where data is being stored or transmitted.


# Setting up a local environment 
Master Node 
Worker Nodes (Slaves)
https://kubernetes.io/docs/concepts/overview/components/

- AWS (EKS)
- GCP (GKE)
- Azure (AKS)
- Kubeadm https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
- kops https://kops.sigs.k8s.io/

- Minikube 
- A Minikube cluster is only intended for testing purposes, not for production. 

- Kubernetes CLI - kubectl 

To run minikube as a docker container, you need the below command: 
# minikube start --driver=docker

You can verify that the cluster is created successfully with: 
# kubectl cluster-info

If things go wrong, you may want to delete and restart minikube by running: 
# minikube delete 


https://github.com/stefanprodan/podinfo

# kubectl get pods 
It lists all the pods aka applications running in your cluster in the given namespace. 
If you have not created any namespace, everything runs in your default namespace. 

# kubectl run nginx --image=nginx
- kubectl run nginx 
  This deploys an app in the cluster and give it a name, nginx. 
- --image=nginx 
  This states the docker image that is being used to spawn the application or container. 

# kubectl run podinfo --restart=Never --image=ghcr.io/stefanprodan/podinfo:6.2.2 --port=9898

# kubectl delete pod <podName>
This deletes the specified pod. 

# kubectl expose pod podinfo --port=9898 --type=NodePort 
- kubectl expose pod podinfo
  This statement exposes the pod named podinfo 
- --port=9898 
  This is the port exposed by the container. 
- --type=NodePort 
  This is the type of service that is being used. 
  We are using the NodePort service to expose the app to external traffic. 

https://kubernetes.io/docs/concepts/services-networking/service/

# kubectl get services 
# kubectl get svc 
This will list out all the services available in your cluster. 

# minikube service --url podinfo 
This command makes it possible for you to access the application over a browser on your local
machine. 