Course: Kubernetes for DevOps Engineers. 
Tutor: Samuel
Date: June 24, 2025
Lesson: Introduction to Kubernetes

# Introduction to Kubernetes

What is Kubernetes?
Kubernetes, often abbreviated as K8s, is an open-source platform designed to automate the deployment, 
scaling, and management of containerized applications. Originally developed by Google and now maintained 
by the Cloud Native Computing Foundation (CNCF), Kubernetes has become the industry standard for container orchestration.

# Evolution Of Application Deployments
Monolith Vs Microservices

Monolith is application built as one entity. Tightly coupled applications.
# Advantages
1. Simpler to manage.Testing becomes easier.
2. Faster deployment times and great for small projects.

# Disadvantages
1. The application fails all at once when one service fails.
2. Scalability becomes complex. 
3. Any changes would require rebuilding  and redeployment of the entire application 

# Microservices
Microservices breaks down application into small independent services, responsible for specific business capability. These services could include backend payment services,login systems,password storage etc.

# Advantage
1. Scalable for large projects.
2. Each service can be tested independently.

# Disadvantages
1. High Complexity bring difficulty in management of application.
2. Cost Intensive.


- The Problem Kubernetes Solves
Before Kubernetes, deploying applications at scale was a complex and error-prone process. Organizations faced several challenges:
1. Manual Deployment and Scaling: Without automation, deploying applications across multiple servers required significant manual effort. Scaling up or down to meet demand was slow and inefficient.
2. Resource Utilization: Optimizing resource usage was difficult. Servers often ran under capacity, wasting computational resources and increasing costs.
3. Application Resilience: Ensuring high availability and fault tolerance demanded complex configurations. Recovering from failures required manual intervention, leading to downtime.
4. Environment Consistency: Discrepancies between development, testing, and production environments led to the classic "it works on my machine" problem, causing deployment issues.
5. Complex Networking and Service Discovery: Configuring networking between services was intricate, and enabling services to discover each other dynamically was challenging.

- Kubernetes addresses these problems by providing:
1. Automated Deployment and Scaling: It automates the distribution of application containers across a cluster and adjusts the number of running containers based on demand.
2. Efficient Resource Management: Kubernetes schedules workloads based on resource requirements and constraints, maximizing utilization.
3. Self-Healing Mechanisms: It automatically restarts failed containers, replaces nodes, and kills unresponsive containers to maintain desired state.
4. Consistent Environments: Containers encapsulate applications and their dependencies, ensuring consistency across different environments.
5. Simplified Networking and Service Discovery: Kubernetes offers built-in service discovery and load balancing, simplifying communication between services.

- How Kubernetes Makes Life Easier for Developers and Engineers

For Developers:
1. Focus on Application Development: Developers can concentrate on writing code without worrying about the underlying infrastructure or deployment complexities.
2. Consistent Deployment Environment: Containers ensure that applications run the same way in development, staging, and production, reducing environment-related bugs.
3. Simplified CI/CD Pipelines: Kubernetes integrates well with continuous integration and continuous deployment tools, streamlining the release process.
4. Scalable Applications: Applications can handle increased load seamlessly, as Kubernetes manages scaling automatically.

For Operations Engineers:
1. Automated Operations: Routine tasks like deployment, scaling, and updates are automated, reducing manual workload and the potential for human error.
2. Efficient Resource Utilization: Kubernetes optimizes the use of infrastructure resources, which can lead to cost savings.
3. High Availability and Resilience: Built-in self-healing and replication features ensure that applications remain available, even in the face of failures.
4. Flexibility and Portability: Kubernetes runs on various environments—on-premises, cloud, or hybrid setups—providing flexibility in infrastructure choices.
5. Enhanced Security: Kubernetes offers features like secrets management and role-based access control (RBAC), helping to secure applications and infrastructure.

# Kubernetes Architechture
- Master Node(Control Plane) - ApiServer, Control Manager, CloudControl Manager, etcd,Scheduler
- Worker Node - Container-runtime, kubelet, kube-proxy

The Kube proxy manages the the network rules for  the communication between the pods and services.

# How It All Works Together
- You tell Kubernetes: "Run my app!"
- The API server gets your request
- The scheduler picks a worker node
- The kubelet on that node starts your app inside a pod
- The controller keeps checking — if the app crashes, Kubernetes restarts it
- You access the app through a Service, like a phone number

Credit : ChatGPT

# Core Kubernetes Components
- Pod - the smallest unit in k8s. An abstraction of a containerised application.
        pods are ephemeral (they die out easily)
- Service - an abstraction over ip addresses. static ip address.
      Types of Service - NodePort, LoadBalancer, ClusterIp

    NB: The lifecycle of of the service and pods are not connected. - If Pod crashes, the service and its IP address will be the same.

# Kubernetes Commands
- PODS - kubectl run <resourceName> --image=<imagename>
        kubectl run podinfo --image=ghcr.io/stefanprodan/podinfo:6.2.2

- Service - # kubectl expose pod podinfo --port=9898 --type=NodePort 
- kubectl expose pod podinfo

  This statement exposes the pod named podinfo 
- --port=9898 
  This is the port exposed by the container. 
- --type=NodePort 
  This is the type of service that is being used. 
  We are using the NodePort service to expose the app to external traffic. 


  - In kubernetes, traffic routing is done by labels and selectors on the pods and services.
  Pods have labels and service has selectors. For traffic to be routed to the right pod labels on the pods must match the service selectors.
  
  # kubectl get {k8s-component} - get status of components
  - kubectl get nodes - gets the node in the cluster
  - kubectl get service - gets the services in the namespace
  - kubectl get pod - gets the pods in the namespace
  - kubectl get all - gets all resources in cluster
  - kubectl get deployment - gets deployment in the namespace
  - kubectl get namespace - gets all namespaces

  # kubectl create {k8s-component} {name} {options} - create components
   - kubectl create deployment nginx-deployment --image=nginx

  # kubectl edit {k8s-component} {name} - edit components
  # kubectl delete {k8s-component} {name} - delete components

  # Debugging
  - kubectl logs {pod-name}
  - kubectl describe {k8s resource}

  - kubectl exec -it {pod-name} -- bash

  - kubectl apply -f {config-file.yaml}

  # Switching namespaces
  - kubectl config set-context --current --namespace=jomacsit

  - https://github.com/ahmetb/kubectx - this installs kubens


# Difference between kubectl create and apply commands:

1. kubectl create: This command is used to create a new resource. 
You would typically use it with a YAML file that defines the resource, like so: 
kubectl create -f pod.yaml. The create command is imperative, 
which means it directly makes changes to the cluster state.

A key limitation is that kubectl create doesn't provide a built-in solution for updating existing resources. 
If you try to create a resource that already exists, you'll get an error.

2. kubectl apply: 
This command can be used to create or update resources. 
Like kubectl create, you typically use it with a YAML file: 
kubectl apply -f pod.yaml. However, apply is declarative, which means 
you define in the file what you want the state of the resource to be, 
and Kubernetes works out what needs to be done to achieve that state.

One of the key features of kubectl apply is that it maintains a record of the previous 
configuration when it updates a resource. This means that it can manage updates to resources 
that it didn't originally create.


# Kubernetes manifest files

eg. 

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: NodePort

---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
spec:
  matchLabels:
    app: nginx
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80


# Basic parts of k8s manifest file

- apiVersion: specifies the api version
- kind: specifies the kind of component to be created
- metadata: a brief info about the component
- spec: the specification of the component to be created



- Ingress

- Deployment(Replicasets)

- Statefulset

- Daemonsets

- ConfigMap

- Secret

- Volume
